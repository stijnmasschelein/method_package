---
title: "Simulations, Regressions, and Significance"
author: "Stijn Masschelein"
date: "`r format(Sys.Date(), '%B, %Y')`"
output:
    xaringan::moon_reader:
        css: "xaringan-themer.css"
        nature:
            countIncrementalSlides: false
            ratio: 16:9
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'svg')
library(tidyverse)
library(cowplot)
library(viridis)
library(here)
i_am("slides/slides2.Rmd")
theme_set(theme_cowplot(font_size = 18))
options(tibble.width = 60)
options(width = 60)
uwa_blue = "#003087"
uwa_gold = "#DAAA00"
```

class: center, middle

# Simulations

---

## Why simulate data?

- Visualising your theory
- Experimenting with and understanding statistical tests 
- Experimenting with statistical approaches without peaking at your data.

See also [Chapter 15, The Effect](https://theeffectbook.net/ch-Simulation.html)

???

- Visualising can help you sharpen your intuition for your theory and for which values are reasonable and which are not.
- You can simulate variables and causal structures that you cannot observe. See also this week's homework
- You don't want to just decide on which statistical test to use because it gives you the "right" answer. If you want to experiment with different statistical models, you can do that with simulated data.

---

## Simulations in `R`


```{r random-number}
N <- 1000
random <- tibble(
  normal = rnorm(N, 2, 5),
  uniform = runif(N, 1, 5),
  binomial = rbinom(N, 1, .25),
  sample = sample(1:10, N, replace = T)
)
glimpse(random)
```

---

```{r random-plot1, fig.width = 18}
p1 <- ggplot(random, aes(x = normal)) + geom_density() + ggtitle("normal density")
plot_grid(p1, ncol = 4)
```

---

```{r random-plot2, fig.width = 18}
p1 <- ggplot(random, aes(x = normal)) + geom_density() + ggtitle("normal density")
p2 <- ggplot(random, aes(x = uniform)) + geom_histogram(bins = 50) + ggtitle("uniform histogram")
plot_grid(p1, p2, ncol = 4)
```

---

```{r random-plot3, fig.width = 18}
p1 <- ggplot(random, aes(x = normal)) + geom_density() + ggtitle("normal density")
p2 <- ggplot(random, aes(x = uniform)) + geom_histogram(bins = 50) + ggtitle("uniform histogram")
p3 <- ggplot(random, aes(x = binomial, y = normal)) + geom_point() + ggtitle("binomal-normal")
plot_grid(p1, p2, p3, ncol = 4)
```

---

```{r random-plot4, fig.width = 18}
p1 <- ggplot(random, aes(x = normal)) + geom_density() + ggtitle("normal density")
p2 <- ggplot(random, aes(x = uniform)) + geom_histogram(bins = 50) + ggtitle("uniform histogram")
p3 <- ggplot(random, aes(x = binomial, y = normal)) + geom_point() + ggtitle("binomal-normal")
p4 <- ggplot(random, aes(x = as.factor(sample), y = uniform)) + geom_jitter(width = .2) +
    ggtitle("sample-uniform") + labs(x = "sample")
plot_grid(p1, p2, p3, p4, ncol = 4)
```

---

class: center, middle

# CEO-firm matching

---

## New theory

 - Firms have different size and CEOs have different talent.
 - More talented CEOs work for bigger firms.
 - Firms pay just enough so that the CEO is not tempted to work for a
 smaller firm.

The model is the second one presented in Edmans and Gabraix
(2016). A more rigorous proof is shown in
[Tervio (2008)](https://www.aeaweb.org/articles?id=10.1257/aer.98.3.642)

A simplified explanation is in [Chapter 5 of the lecture notes](https://stijn-masschelein.netlify.app/teaching/just_enough/maths-sim.htm)

---

## Visualisation of matching theory

.pull-left[

```{r matching-viz, echo = FALSE}
obs <- 500
size_rate <- 1; talent_rate <- 2/3;
C <- 10; scale <- 600; w0 = 1;
n <- c(1:obs)
size <- scale * n ^ (-size_rate)
talent <- - 1/talent_rate * n ^ (talent_rate)
wage <- rep(NA, obs)
wage[obs] <- w0
for (i in (obs - 1):1){
  wage[i] <- wage[i + 1] + 1/scale * C * size[i + 1] *
      (talent[i] - talent[i + 1])
}

matching_plot <- qplot(x = size, y = wage) +
    labs(x = "Company Market Value", y = "CEO compensation")
plot(matching_plot + ggtitle("Value - Compensation"))
```

]

.pull-right[

```{r matching-log, echo = FALSE}
plot(matching_plot +
     scale_x_continuous(trans = "log",
                        breaks = scales::log_breaks(n=5, base=10)) +
     scale_y_continuous(trans = "log",
                        breaks = scales::log_breaks(n=5, base=5)) +
     ggtitle("log(Value) - log(Compensation)")
     )
```

]

---

## CEO compensation data

```{r}
us_comp <- readRDS(here("data", "us-compensation-new.RDS")) %>%
  rename(total_comp = tdc1)
us_value <- readRDS(here("data", "us-value-new.RDS")) %>%
  rename(year = fyear, market_value = mkvalt)
summary(us_value$market_value)
```

---

## Putting it all together

```{r}
us_comp_value <-
    select(us_comp, gvkey, year, total_comp) %>% 
    left_join(
        us_value,
        by = c("year", "gvkey"))
glimpse(us_comp_value)
```

---

## First plot - The basics

.pull-left[

```{r}
plot_comp_value <- ggplot(
    us_comp_value,
    aes(y = total_comp, x = market_value)) +
    geom_point(alpha = .10) +
    ylab("compensation ($ 000)") +
    xlab("market value ($ million)")
```

]

.pull-right[

```{r, fig.height = 5}
print(plot_comp_value)
```
    
]

---

## First plot - The scales 

.pull-left[

```{r}
plot_log <- plot_comp_value +
    scale_x_continuous(
        trans = "log1p",
        breaks = c(1e2, 1e3, 1e4, 1e5, 1e6),
        labels = function(x)
            prettyNum(x/1000, digits = 2)) +
    scale_y_continuous(
        trans = "log1p",
        breaks = c(1e1, 1e2, 1e3, 1e4, 1e5),
        labels = function(x)
            prettyNum(x/1000, digits = 2)) +
    ylab("compensation ($ million)") +
    xlab("market value ($ billion)")
```

]

.pull-right[

```{r, fig.height = 5}
print(plot_log)
```
    
]
---

## First plot - Zoom in

.pull-left[

```{r}
plot_zoom <- plot_log +
    coord_cartesian(
        xlim = c(1e1, NA), ylim = c(1e2, NA))
```

]

.pull-right[

```{r, fig.height = 5}
print(plot_zoom)
```

]

---

class: middle, center

# Linear regression in `R`

See [Chapter 6 of the lecture notes](https://stijn-masschelein.netlify.app/teaching/just_enough/linear-regression.html)

---

## Notation 

.pull-left[

\begin{equation}
y_i = a + b_1 x_{1i} + ... + b_n x_{ni} + \epsilon_i
\end{equation}

\begin{equation}
\vec{y} = a + b_1 \vec{x_1} + ... b_n \vec{x_n} + \vec{\epsilon}
\end{equation}

\begin{equation}
\vec{y} = a + \vec{b} X + \vec{\epsilon}
\end{equation}

\begin{equation}
\vec{y} = \mathcal{N}(a + \vec{b} X, \sigma)
\end{equation}

See also [Chapter 13, The Effect](https://theeffectbook.net/ch-StatisticalAdjustment.html)

]

--

.pull-right[

```{r, eval = FALSE, echo = TRUE}
reg <- lm(y ~ x1 + x2, data = my_data_set)
summary(reg)
```

]


---

### Finally, the regression

```{r, echo = TRUE}
reg = lm(log(total_comp + 1) ~ log(market_value + 1), 
         data = us_comp_value)
# summary(reg)
print(summary(reg)$coefficients, digits = 2)
```
---

class: center, middle

# CEO Incentives and Size

See [Chapter 7 of the lecture notes](https://stijn-masschelein.netlify.app/teaching/just_enough/measurement.html)

---

## Historical Discussion

> Our estimates of the pay-performance relation (including pay, options, stockholdings, and dismissal) for chief executive officers indicate that CEO wealth changes $3.25 for every $1,000 change in shareholder wealth. [(Jensen and Murphy, 1990)](https://www.journals.uchicago.edu/doi/10.1086/261677)

--

> [...] The statistic in isolation can present a misleading picture of pay to performance relationships because the denominator - the change in firm value - is so large. [(Hall and Liebman, 1998)](https://doi.org/10.1162/003355398555702)

--

> This article addresses four major concerns about the pay of U.S. CEOs: (1) failure to pay for performance; [...]. The authors' main message is that most if not all of these concerns are exaggerated by the popular tendency to focus on the annual income of CEOs (consisting of salary, bonus, and stock and option grants) while ignoring their existing holdings of company equity. [(Core, Guay, and Thomas , 2005)](https://doi.org/10.1111/j.1745-6622.2005.00063.x)

---

## Stock holding data

```{r}
us_comp <- readRDS(here("data", "us-compensation-new.RDS")) %>%
    rename(total_comp = tdc1, shares = shrown_tot_pct) %>%
    select(gvkey, execid, year, shares, total_comp)
us_value <- readRDS(here("data", "us-value-new.RDS")) %>%
    rename(year = fyear, market_value = mkvalt) %>%
    select(-ni) 
us_comp_value <- left_join(
    us_comp, us_value, by = c("year", "gvkey")) %>%
    filter(!is.na(market_value) & !(is.na(shares))) %>%
    mutate(wealth = shares * market_value / 100)
glimpse(us_comp_value)
```

---

## Shares to Market Value

.pull-left[

```{r}
plot_shares <- ggplot(
    data = us_comp_value,
    aes(x = market_value/1000, y = shares)) +
    geom_point(alpha = .10) +
    ylab("CEO Ownership") +
    xlab("Firm Market Value (in Billions)") +
    scale_x_continuous(
        trans = "log",
        labels = function(x)
            prettyNum(x, digits = 2),
        breaks =
            scales::log_breaks(n = 5,
                               base = 10)) +
    scale_y_continuous(
        trans = "log",
        labels =
            function(x)
                prettyNum(x, digits = 2),
        breaks =
            scales::log_breaks(n = 5,
                               base = 10))
```
]

.pull-right[
```{r, warning = FALSE, fig.height = 5}
print(plot_shares)
```
]

---

## Pay to Performance Sensitivity

```{r}
us_sens <- us_comp_value %>%
    group_by(gvkey, execid) %>%
    arrange(year) %>%
    mutate(prev_market_value = lag(market_value),
            prev_wealth = lag(wealth)) %>%
    ungroup() %>%
    mutate(change_log_value = log(market_value) - log(prev_market_value),
           change_log_wealth = log(wealth) - log(prev_wealth)) %>%
    filter(!is.infinite(change_log_wealth)) %>%
    arrange(gvkey)
```

???

- The assumption for pay-for-performance and incentives is that we
  want to measure whether a CEO has taken the correct decisions. In a
  bigger firm, the impact of a CEOs decisions are larger. If you
  improve management of employees, then the effects will be bigger for
  a firm with more employees. 
- The other assumption is that CEOs do not care about dollar increases
  in dollars but in increases in percentages. Partly

$$
\begin{align}
\frac{\partial W}{W} \frac{V}{\partial V}  \\
  &= \frac{ln(W)}{ln(V)}
\end{align}
$$

---

.pull-left[

```{r}
plot_hypothesis = ggplot(
    us_sens,
    aes(y = change_log_wealth / change_log_value,
        x = market_value/1000)) +
  geom_point(alpha = .1) +
  scale_x_continuous(
    trans = "log", 
    breaks = scales::log_breaks(n = 5, base = 10),
    labels = function(x) prettyNum(x, dig = 2)) +
  coord_cartesian(
    ylim = c(-10, 10)) +
  xlab("market value") +
  ylab("sensitivity")
```

]

.pull-right[

```{r}
print(plot_hypothesis)
```

]

---

class: center, middle

# p-values

---

## Randomisation or Permutation Test (1)

```{r}
data_hypo <- us_sens %>%
    mutate(
      sensitivity = change_log_wealth / change_log_value) %>%
  select(sensitivity, market_value) %>%
  filter(complete.cases(.))

observed_cor <- cor(
  data_hypo$sensitivity, data_hypo$market_value)

random_cor <- cor(
  data_hypo$sensitivity, sample(data_hypo$market_value))

print(prettyNum(c(observed_cor, random_cor), dig = 3))
```

---

## Randomisation or Permutation Test (2)

.pull-left[

```{r}
simulate_cor <- function(data){
    return(cor(data$sensitivity,
               sample(data$market_value)))}
rand_cor <- replicate(1e4,
                      simulate_cor(data_hypo))
```

```{r}
hist_sim <- ggplot(
    mapping = aes(
        x = rand_cor,
        fill = abs(rand_cor) < abs(observed_cor))) +
    geom_histogram(bins = 1000) +
    xlab("Random Correlations") +
    scale_fill_manual(values = c(uwa_blue, uwa_gold)) +
    theme(legend.position = "none") +
    coord_cartesian(
        xlim = c(-0.1, 0.1))
```

]

--

.pull-right[

```{r, fig.height = 5}
plot(hist_sim)
```

]

---

## Bootstrap

.pull-left[

```{r}
calc_corr <- function(d){
  n <- nrow(d)
  id_sample <- sample(1:n, size = n,
                      replace = TRUE)
  sample <- d[id_sample, ]
  corr <- cor(sample$sensitivity,
              sample$market_value)
  return(corr)
}
boot_corr <- replicate(
    2000, calc_corr(data_hypo))
```

```{r}
plot_boot <- ggplot(
    mapping = aes(x = boot_corr)) +
  geom_histogram(bins = 100, colour = uwa_blue,
                 fill = uwa_blue) +
    geom_vline(aes(xintercept = 0),
               colour = uwa_gold) +
    xlab("Bootstrapped Correlation")
```

]

--

.pull-right[
```{r, fig.height = 5}
print(plot_boot)
```
]

---

## Comparison

.pull-left[

### Permutation Test
- Calculate the observed statistic
- Randomly resample the data by breaking the relation you want to test
  (= Null Hypothesis)
- Calculate the statistic for each random sample
- Is the observed statistic more extreme than the randomly resampled
  statistic?

See also [Chapter 4.2, Causal Mixtape](https://mixtape.scunning.com/potential-outcomes.html#randomization-inference)

]

--

.pull-right[

### Bootstrap

- Randomly sample observed observations with replacement. 
- Calculate the statistic you are interested in.
- Is the distribution of resampled statistics unlikely to be 0 (= Null
  Hypothesis)?

See also [Chapter 15, The Effect](https://theeffectbook.net/ch-Simulation.html#simulation-with-existing-data-the-bootstrap)

]

---

## Formula Based P-value

```{r}
cor <- cor.test(data_hypo$sensitivity, data_hypo$market_value)
pvalue_cor <- cor$p.value
print(prettyNum(pvalue_cor, dig = 2))
```

--

```{r}
regr_sens <- lm(sensitivity ~ I(market_value/1e3), data = data_hypo)
coefficients(summary(regr_sens)) %>% print(dig = 2)
```

---

class: center, middle

# Pitching Template

---

## Pitching Format

1. Description (Important)
    - Title
    - Research Question
    - Key Paper
    - Motivation
2. THREE (IDioT) (Important)
    - Idea 
    - Data 
    - Tools 
3. TWO 
    - What's new?
    - So what?
4. ONE contribution
5. Other considerations.

---

class: center, middle

# Assignment 1: Plots and regressions

---

class: center

## Models

.pull-left[


#### Signalling Model 

![Peacock](https://www.arts.uwa.edu.au/__data/assets/image/0010/117973/Peacock-165px.jpg)

]

--

.pull-right[

#### Cheap Talk Model

![Assumptions](https://www.dlsweb.rmit.edu.au/lsu/content/B_DSC/gsssp/graphics/iceberg.jpg)

]

---

## Answers

```{r, eval = FALSE}
N <- 1000
high_performance <- rbinom(.x, .y, .z)
donation <- ifelse(.x, 1, 0)
return <- ifelse(donation == 1, .y, .z)
observed_donation <- ifelse(rbinom(N, 1, .9) == 1, donation, 1 - donation)
observed_return <- ... 
sig <- tibble(return = ...,
              donation = ...) %>%
    mutate(donated = ...)
glimpse(sig)
```

--

```{r, eval = FALSE}
sig_plot <- ggplot(..., aes(x = .x, y = .y)) +
    geom_jitter(width = .3)
plot(sig_plot)
```

--

```{r, eval = FALSE}
sig_reg <- lm(..., data = sig)
summary(...)
```
